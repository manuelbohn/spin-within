---
title             : "Modeling individual differences in children's information integration during pragmatic word learning"
shorttitle        : "Modeling individual differences in developmental pragmatics"

author: 
  - name          : "Manuel Bohn"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, 04103 Leipzig, Germany"
    email         : "manuel_bohn@eva.mpg.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Methodology
      - Formal Analysis
      - Visualization
      - Writing – original draft
      - Writing – review & editing
  - name          : "Louisa S. Schmidt"
    affiliation   : "2"
    role:
      - Conceptualization
      - Methodology
      - Investigation
      - Writing – original draft
      - Writing – review & editing
  - name          : "Cornelia Schulze"
    affiliation   : "2,3"
    role:
      - Conceptualization
      - Methodology
      - Writing – review & editing
  - name          : "Michael C. Frank"
    affiliation   : "4"
    role:
      - Conceptualization
      - Writing – review & editing
  - name          : "Michael Henry Tessler"
    affiliation   : "5,6"
    role:
      - Conceptualization
      - Methodology
      - Formal Analysis
      - Writing – review & editing

      
affiliation:
  - id            : "1"
    institution   : "Department of Comparative Cultural Psychology, Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany"
  - id            : "2"
    institution   : "Leipzig Research Center for Early Child Development, Leipzig University, Leipzig, Germany"
  - id            : "3"
    institution   : "Department of Educational Psychology, Faculty of Education, Leipzig University, Leipzig, Germany"
  - id            : "4"
    institution   : "Department of Psychology, Stanford University, Stanford, USA"
  - id            : "5"
    institution   : "DeepMind, London, UK"
  - id            : "6"
    institution   : "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, USA"

authornote: |
  M. H. Tessler was funded by the National Science Foundation SBE Postdoctoral Research Fellowship Grant No. 1911790. M. C. Frank was supported by a Jacobs Foundation Advanced Research Fellowship and the Zhou Fund for Language and Cognition. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.

abstract: |
  Pragmatics is foundational to language use and learning. Computational cognitive models have been successfully used to predict pragmatic phenomena in adults and children -- on an aggregate level. It is unclear if they can be used to predict behavior on an individual level. We address this question in children (N = 60, 3- to 5-year-olds), taking advantage of recent work on pragmatic cue integration. In Part 1, we use data from four independent tasks to estimate child-specific sensitivity parameters to three information sources: semantic knowledge, expectations about speaker informativeness, and sensitivity to common ground. In Part 2, we use these parameters to generate participant-specific trial-by-trial predictions for a new task that jointly manipulated all three information sources. The model accurately predicted children’s behavior in the majority of trials. This work advances a substantive theory of individual differences in which the primary locus of developmental variation is sensitivity to individual information sources.  
  
keywords          : "Pragmatics, language development, individual differences, cognitive modeling"
#wordcount         : "X"

bibliography      : "../../../References/library.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
keep_tex          : true
---

```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(matrixStats)
library(BayesFactor)

flip <- function(x){
  coin <- sample(c(0,1), size = 1, replace = TRUE, prob = c(1 - x,x))
  return(coin)
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r}
data <- read_csv("../data/merged_data.csv")
```

# Statement of Relevance

Formal computational models provide a promising tool to overcome the weaknesses of verbal theories: they explicate the psychological processes that generate behavior. Probabilistic models have been especially useful for characterizing how some human behaviors can be described as optimal use of environmental information. However, models of this type often turn a blind eye to individual differences: Explanations are offered for prototypical agents, and predictions are tested against aggregated data representing the “average” individual. Here, we applied computational modeling to an individual level. We first measured children’s sensitivity to three information sources and then used a computational model to generate trial-by-trial predictions about how each child should behave in a new task that required information integration. We correctly predicted children’s behavior in the majority of trials. This success suggests that, with sufficiently reliable measures, probabilistic models may be theories of individuals’ judgments as well as behavior in the aggregate. 

\newpage

# Introduction 

A defining feature of human communication is its flexibility. Conventional languages -- signed and spoken -- allow for expressing a near-infinite number of messages. In the absence of a shared language, humans can produce and understand novel signals which can rapidly be transformed into structured communication systems [@bohn2019young; @brentari2017language]. The flexibility stems from a powerful social-cognitive infrastructure that underlies human communication [@tomasello2008origins; @sperber2001relevance]. Interlocutors can recruit and integrate a range of different information sources -- conventional language being one of them -- to make so-called *pragmatic* inferences about the speaker’s intended meaning in context. They play an important role during everyday language use [@clark1996using] and during language acquisition [@bohn2019pervasive; @clark2009first]. 

Decades of developmental research have shown that children readily make pragmatic inferences in a wide variety of contexts and starting at an early age [@bohn2019pervasive]. For example, already early in the second year of life, children use their emerging semantic knowledge (word-object mappings) to infer that a speaker uses a novel word to refer to a novel object [@pomiechowska2021twelve; @markman1988children]. Around the same age, children start to use common ground (shared knowledge) in communication [@bohn2018common]. From age three onwards, they expect speakers to communicate in an informative and context-sensitive way [@frank2014inferring; @schulze20133]. 

Theoretical accounts of language use and learning postulate that these pragmatic inferences require integrating various sources of information, but often fail to specify how exactly the information integration happens. This theoretical paucity is a special case of a more general issue in psychology and -- specifically — in developmental science, where there is a lack of strong, explicit theories that predict and explain behavior [@muthukrishna2019problem]. Computational cognitive modeling is one way to overcome this issue [@van2021theory; @simmering2010dialogue]. Cognitive models formalize the computational processes that generate the observed behavior [@van2022psychological; @ullman2020bayesian]. The modeling process forces researchers to state explicitly their assumptions and intuitions, which can result in stronger theories [@guest2021computational].

The field of pragmatic language comprehension has been particularly active from a computational modeling perspective [@cummins2014computational], including work on common ground [@anderson2021tell; @heller2016perspective], politeness [@yoon2020polite]; over-informativeness [@degen2020redundancy]; implicature [@franke2020theory], and generic language [@tessler2019language]. The Rational Speech Act (RSA) framework has been one productive framework for modeling pragmatic inference, construing language understanding as a special case of Bayesian social reasoning (Frank & Goodman, 2012; Goodman & Frank, 2016). RSA models are distinguished by their recursive structure in which a listener reasons about a cooperative speaker -- sensu @grice1991studies -- who reasons about a literal listener who interprets words according to their literal semantics. These models have been successfully applied to predict aggregate behavior -- the average judgment probability across a large group of participants, for example -- for a range of different pragmatic phenomena [reviewed in @frank2012predicting; @goodman2016pragmatic].

Computational cognitive models -- including RSA -- are mostly used as summary descriptions and explanations of well-known effects from the literature or in pre-existing data. Yet, for a comprehensive theory, models should also be able to *predict* new data [@yarkoni2017choosing; @hofman2021integrating; @shmueli2010explain]. Recent work using RSA models has begun to address this issue. For example, @bohn2021young studied young children’s information integration during pragmatic word learning [see also @bohn_tessler_merrick_frank_2019]. They measured children’s developing sensitivity to three different sources of information about meaning in context and used an RSA model to generate predictions about situations in which these information sources need to be integrated. Newly collected data aligned closely with what the model predicted, in the sense that the model predictions were numerically similar to the average level of performance across a large sample of children. This line of work tested the scope and validity of models of pragmatic reasoning and the results offered support for the theoretical assumptions around which the model was built in comparison to alternative models.

These prior studies only explained and predicted behavior on an *aggregate* level, however. The models were assessed following the assumption that the "average person" behaves like the prototypical agent whose cognitive processes are being simulated by the model [@estes2005risks]. Yet it is an open question if everybody -- or in fact anybody -- really behaves like this prototypical agent. Most likely, there are differences between individuals. For example, @franke2016reasoning studied quantity implicatures and found that participant data was best captured by a model that assumes a population in which individuals differ in the depth of their Theory of Mind reasoning. A central question is therefore whether models that accurately predict group-level results can also be used to predict individual differences. For example, although @griffiths2006optimal showed that groups of participants in the aggregate could correctly make optimal judgments about the conditional probability of everyday events, @mozer2008optimal argued that this pattern could emerge from an aggregate of individual agents with far simpler and more heuristic strategies [cf. @griffiths2011predicting]. Thus, the fit of cognitive models to aggregate patterns of data may not always support the inference that the cognitive model describes individuals’ patterns of reasoning or inference.

In the present study, we address this issue in the domain of pragmatic word learning, using RSA models to predict individual differences between children. Our study builds on @bohn2021young and measures how children integrate different information sources. We focused on how children’s semantic knowledge interacts with their expectations about informative communication and sensitivity to common ground. Following the previous study, we formalized this integration process in a model derived from the RSA framework. Importantly, however, the current model was designed to capture individual differences, which we conceptualize as differences between children in sensitivity to the different information sources. In Part 1, we collected data in four tasks from which we estimated child-specific sensitivity parameters. In Part 2, we used these parameters to predict -- on a trial-by-trial basis -- how the same children should behave in a new task that required information integration. The critical contribution of this work is thus to test whether a successful model of aggregate judgments holds at the individual level.


# Part 1: Sensitivity

## Methods

Methods, sample size, and analyses were pre-registered at: https://osf.io/pa5x2/?view_only=00d4d608010548b18aade9cf40909a71. All data, analysis scripts, model code, and experimental procedures are publicly available in the following online repository: [github link masked for peer review - link to repository hosted on OSF: ]. 

### Participants

```{r}
dem <- data%>%
  distinct(subid, .keep_all = T)%>%
  summarise(n = n_distinct(subid),
            female = sum(sex == "f"),
            mage = mean(age_num),
            lrange = range(age_num)[1],
            urange = range(age_num)[2])
```

We collected complete data for `r dem%>%pull(n)` children ($m_{age}$ = `r dem%>%pull(mage)`, range$_{age}$: `r dem%>%pull(lrange)` - `r dem%>%pull(urange)`, `r dem%>%pull(female)` girls) during two experimental sessions each. As per our pre-registration, children who provided valid data for fewer than half of the test trials in any of the three experiments were excluded from the analysis. This was the case for five additional children (two 3-year-olds, three 4-year-olds) due to disinterest in the experiments (n = 2), parental interference due to fussiness (n = 2), or withdrawal from the study after the first testing session (n = 1). Children came from an ethnically homogeneous, mid-size German city (~550,000 inhabitants, median income €1,974 per month as of 2020), were mostly monolingual, and had mixed socioeconomic backgrounds. The study was approved by an internal ethics committee at the Max Planck Institute for Evolutionary Anthropology. Data was collected between March and July of 2021.

### Measures

Children were recruited via a database and participated with their parents via an online conferencing tool. The different tasks were programmed as interactive picture books in `JavaScript/HTML` and presented on a website. During the video call, participants would enter the website with the different tasks and share their screens. The experimenter guided them through the procedure and told caregivers when to advance to the next task. Children responded by pointing to objects on the screen, which their caregivers would then select for them via mouse click. For the word production task, the experimenter shared their screen and presented pictures in a slide show. For the mutual exclusivity, discourse novelty, and combination tasks (Part 2), pre-recorded sound files were used to address the child. Figure \@ref(fig:fig1) shows screenshots from the different tasks.

The *discourse novelty* task assessed children's sensitivity to common ground (see Figure \@ref(fig:fig1)). Children saw a speaker (cartoon animal) standing between two tables. On one table, there was a novel object (drawn for the purpose of this study) while the other was empty (side counterbalanced). The speaker sequentially turned to both sides (order counterbalanced) and either commented on the presence or absence of an object (without using any labels, see supplementary material for details). Then, the speaker disappeared, and -- while the speaker was gone -- another novel object appeared on the previously empty table. Next, the speaker re-appeared and requested one of the objects using a novel non-word as the label. We assumed that children would take the novel word to refer to the object that was new to the speaker. Children received 12 trials, each with a new pair of novel objects. 

The *mutual exclusivity* task was used to assess children’s semantic knowledge and expectations about speaker informativeness (see Figure \@ref(fig:fig1). Children again saw a speaker and two tables. On one table, there was a novel object while on the other there was a (potentially) familiar object (side counterbalanced). The speaker used a novel non-word to request one of the objects. We assumed that children would take the novel word to refer to the novel object. In line with previous work [@grassmann2015children; @bohn2021young; @lewis2020role] we assumed this inference would be modulated by children’s lexical knowledge of the familiar object. Children received 16 trials, each with a new pair of novel and familiar objects. Both the discourse novelty as well as the mutual exclusivity tasks showed good re-test reliability (*r*  > .7 for both tasks) in a previous study and seem well-suited for individual-level measurement [@bohn2022individual].

The *word production* task assessed children’s semantic knowledge (see Figure \@ref(fig:fig1). The experimenter showed the child each of the 16 familiar objects from the mutual exclusivity task and asked them to name them. We used a pre-defined list of acceptable labels per object to categorize children’s responses as either correct or incorrect (see supplementary material).

The *word comprehension* task was also used to assess semantic knowledge (see Figure \@ref(fig:fig1). The child saw four slides with six objects. Four objects per slide were taken from the 16 familiar objects that also featured in the mutual exclusivity and word production tasks. Two objects were unrelated distractors. The experimenter labeled one familiar object after the other and asked the child to point to it. 

Data collection for the entire study (Part 1 and 2) was split into two sessions which took place around one week apart (min: 1 day, max: 2 weeks). On day one, children completed the mutual exclusivity and the discourse novelty tasks. On day two, they completed the combination task (Part 2) followed by the word comprehension and production tasks.

```{r fig1, out.width="100%", fig.cap = "Schematic overview of the study and the model. Pictures on the left show screenshots from the four sensitivity tasks. Arrows indicate which tasks informed which parameter in the model (grey area). Based on the data from the sensitivity tasks, child-specific parameter distributions for each information source were estimated. These sources were integrated via an RSA model, which generated predictions for each trial of the combination task. These predictions were then evaluated against new data from the combination task."}
knitr::include_graphics("./figures/fig1.png")
```

## Analysis

The goal of the analysis of Part 1 was to estimate participant-specific sensitivity parameters based on the tasks described above. Parameter estimation happens in the context of the modeling framework we used to generate predictions for the novel task in Part 2. In the following, we first describe the general modeling framework and then continue with the participant-specific parameter estimation.

### Modeling framework

We adopted the modeling framework used by @bohn2021young. Our models are situated in the Rational Speech Act (RSA) framework [@frank2012predicting; @goodman2016pragmatic]. RSA models treat language understanding as a special case of Bayesian social reasoning. A listener interprets an utterance by assuming it was produced by a cooperative speaker who has the goal to be informative. Being informative is defined as producing messages that increase the probability of the listener inferring the speaker’s intended message. The focal *rational integration* model, including all data-analytic parameters, is formally defined as:

\begin{equation}
P_{L_1}(r \mid u; \{\rho_i, \alpha_i\ , \theta_{ij}\})\propto P_{S_1}(u \mid r; \{\alpha_i, \theta_{ij}\}) \cdot P(r \mid \rho_i)
(\#eq:rsafull1)
\end{equation}

The model describes a listener ($L_1$) reasoning about the intended referent of a speaker’s ($S_1$) utterance. This reasoning is contextualized by the prior probability of each referent $P(r \mid \rho_i)$. This prior probability is a function of the common ground $\rho$ shared between speaker and listener in that interacting around the objects changes the probability that they will be referred to later. We assume that individuals vary in their sensitivity to common ground which, captured in participant-specific parameters $\rho_i$. 

To decide between referents, the listener ($L_1$) reasons about what a rational speaker ($S_1$) would say given an intended referent. This speaker is assumed to compute the informativity for each available utterance and then choose an utterance in proportion to its informativity raised to the power of the parameter $\alpha$. As such, $\alpha$ reflects how informative the listener expects the speaker to be. This expectation may vary between individuals, leading to a participant-specific parameter $\alpha_i$: 

\begin{equation}
P_{S_1}(u \mid r; \{\alpha_i\, \theta_{ij}\})\propto P_{L_0}(r \mid u; \{\theta_{ij}\}) ^{\alpha_i}
(\#eq:rsafull2)
\end{equation}

The informativity of each utterance is given by imagining which referent a literal listener ($L_0$), who interprets words according to their lexicon $\mathcal{L}$, would infer upon hearing the utterance. This reasoning depends on what kind of semantic knowledge (word–object mappings, $\theta$) the speaker thinks the literal listener has. For familiar objects, we take semantic knowledge to be a function of the degree-of-acquisition of the associated word, which we assume to vary between individuals ($\theta_{ij}$).

\begin{equation}
P_{L_0}(r \mid u; \{\theta_{ij}\}) \propto \mathcal{L}(u, r \mid \theta_{ij})
(\#eq:rsafull3)
\end{equation}

This modeling framework describes how different information sources are integrated and how individuals might differ from one another. More specifically, we assume individual differences to arise from varying sensitivities to the three information sources (captured in the participant-specific parameters $\rho_i$, $\alpha_i$, and $\theta_{i,j}$). The process by which information is integrated is thought to follow the same rational (Bayesian) procedure for all participants. Given participant-specific values for the three sensitivity parameters, this model allows us to generate participant-specific predictions for situations in which information needs to be integrated. Next, we describe how we estimated these participant-specific parameter values based on the data collected in Part 1. 

### Parameter estimation

Models to estimate parameters were implemented in the probabilistic programming language `webppl` [@dippl]. As noted above, the three information sources were: sensitivity to common ground ($\rho_i$), expectations about speaker informativeness ($\alpha_i$), and semantic knowledge ($\theta_{ij}$). Figure \@ref(fig:fig1) shows which tasks informed which parameters. All parameters were estimated via hierarchical regression (mixed-effects) models. That is, for each parameter, we estimated an intercept and slope (fixed effects) that best described the developmental trajectory for this parameter based on the available data. Participant-specific parameters values (random effects) were estimated as deviations from the value expected for a participant based on their age. Details about the estimation procedure can be found in the supplementary material and code to run the models can be found in the associated online repository.

The parameters for semantic knowledge ($\theta_{ij}$) were simultaneously inferred from the data from the mutual exclusivity, the comprehension, and the production experiments. To leverage the mutual exclusivity data, we adapted the RSA model described above to a situation in which both objects (novel and familiar) had equal prior probability (i.e., no common ground information). In the same model, we also estimated the parameter for speaker informativeness (see below). 

For the comprehension experiment, we assumed that the child knew the referent for the word with probability $\theta_{ij}$. If $\theta_{ij}$ indicated that they knew the referent (a coin with weight $\theta_{ij}$ comes up heads) they would select the correct picture; if not they would select the correct picture at a rate expected by chance (1/6). Likewise, for the production experiment, we assumed that the child knew the word for the referent with probability $\theta_{ij}$. If $\theta_{ij}$ indicated that they knew the word (a coin with weight $\theta_{ij}$ comes up heads), we assumed the child would be able to  produce it with probability $\gamma$. This successful-production-probability $\gamma$ was the same for all children and was inferred based on the data. This adjustment reflects the finding that children’s receptive vocabulary for nouns tends to be larger than the productive [@clark1983comprehension; @frank2021variability]. Taken together, for each child $i$ and familiar object $j$ there were three data points to inform $\theta$: one trial from the mutual exclusivity, one from the comprehension and one from the production experiment.


The parameter representing a child’s expectations about how informative speakers are ($\alpha_i$), was estimated based on the data from the mutual exclusivity experiment. As mentioned above, this was done jointly with semantic knowledge in a RSA model adopted to a situation with equal prior probability of the two objects (novel and familiar). Thus, for each child, there were 16 data points to inform $\alpha$.

We estimated children’s sensitivity to common ground ($\rho_i$) based on the data from the discourse novelty experiment. This was done via simple logistic regression and based on the 12 data points from this task.

## Results

Figure \@ref(fig:fig2) visualizes the results for the four sensitivity tasks and the participant-specific model parameters estimated from the data. In all four tasks, we saw that children performed above chance (not applicable in the case of word production), suggesting that they made the alleged pragmatic inference or knew (some) of the words for the objects involved. With respect to age, performance in raw test scores seemed to increase with age in the three tasks relying on semantic knowledge (mutual exclusivity, word production and word comprehension). Performance in these tasks was also correlated (see supplementary material). For discourse novelty, performance did not increase with age. Most importantly, however, we saw considerable variation in raw scores between individuals (see Figure \@ref(fig:fig2). When focusing on the participant-specific parameter estimates (Figure \@ref(fig:fig2)B), we saw that parameters that were estimated based on more data (sensitivity to common ground -- 12 trials, and expectations about speaker informativeness -- 16 trials) had better defined posterior distributions in comparison to the semantic knowledge parameters, which were based on fewer data (3 trials per object).

```{r fig2, out.width="100%", fig.cap = "Results for the sensitivity tasks. A: proportion of correct responses in each task by age. Colored dots show the mean proportion of correct responses (with 95\\% CI) binned by year. Regression lines show fitted generalized linear models with 95\\% CIs. B: posterior distributions for each parameter (information source) and participant, ordered by mean value, separate for each parameter. Color shows age group."}
knitr::include_graphics("./figures/fig2_1.png")
```

## Discussion

In  Part 1, we estimated participant-specific parameters representing each individual’s sensitivity to the three information sources. We found that, as a group, children were sensitive to the different information sources we measured. Furthermore, there was substantial variation between individuals in *how* sensitive they were to each information source. These results provided a solid basis for studying information integration in Part 2.

# Part 2: Integration

## Methods

The study was pre-registered and all data, analysis script and materials are publicly available (see Part 1 for more information).

### Participants

Participants were the same as in Part 1. 

### Procedure

The task was implemented in the same environment as the tasks in Part 1. Each child completed the combination task in the second testing session. The general procedure followed that of the discourse novelty task, however, only one of the objects was unknown while the other was familiar. The combination task had two conditions. In the *congruent condition*, the unfamiliar object was also new to discourse. For example, at the beginning of the trial, a familiar object (e.g. a lock) was on one table while the other table was empty. When the agent disappeared, a novel object appeared. When the experimenter returned and used a novel none-word both the mutual exclusivity and discourse inferences pointed to the novel object as the referent of the novel word (see also Figure \@ref(fig:fig1). In the *incongruent condition*, the familiar object was new to discourse and thus the two inferences pointed to different objects (the mutual exclusivity inference would suggest the novel object but the common ground would suggest the familiar object). The idea behind having these different conditions was to increase variability in children’s responses to test the scope of the model. We created matched pairs for the 16 familiar objects and assigned one object of each pair to one of the two conditions. Thus, there were eight trials per condition in the combination task in which each trial was with a different familiar object. We counterbalanced the order of conditions and the side on which the discourse-novel object appeared. Responses were coded from a mutual exclusivity perspective (choosing novel object = 1). All children received the same order of trials. There was the option to terminate the study after 8 trials (two children).

## Analysis

We used the rational integration model described above to generate predictions for each participant and trial in the combination task based on the participant-specific parameters estimated in Part 1. That is, for each combination of $\rho$, $\alpha$, and $\theta$ for participant $i$ and familiar object $j$, the model returned a distribution for the probability with which the child should choose the novel object. 

We contrasted the predictions made by the rational integration model described above to those made by two plausible alternative models which assume that children selectively ignore some of the available information sources [@gagliardi2017modeling]. These models generated predictions based on the same parameters as the *rational integration* model, the only difference lay in how the parameters were used. The *no speaker informativeness* model assumed that the speaker does not communicate in an informative way and therefore focused on the sensitivity to common ground. The *no common ground* model ignores common ground information and focuses on the mutual exclusivity inference (speaker informativeness and semantic knowledge instead). A detailed description of all the models along with technical information about parameter estimation can be found in the supplementary material.

We evaluated the model predictions in two steps. First, we replicated the group-level results of @bohn2021young. That is, we compared the three models in how well they predicted the data of the combination task when aggregated across individuals. For this, we correlated model predictions and the data (aggregated by trial and age group) and computed Bayes Factors comparing models based on the marginal likelihood of the data given the model.

Second, and most importantly, we evaluated how well the model predicted performance on an *individual* level. For each trial, we converted the (continuous) probability distribution returned by the model into a binary prediction (the structure of the data) by flipping a coin with the Maximum a posteriori estimate (MAP) of the distribution as its weight^[Note that this procedure is not deterministic and the results will slightly vary from one execution to the next (see also Figure \@ref(fig:fig4).]. For the focal and the two alternative models, we then computed the proportion of trials for which the model predictions matched children's responses and compared them to a level expected by random guessing using a Bayesian t-test. Finally, for each child, we computed the Bayes Factor in favor of the *rational integration* model and checked for how many children this value was above 1 (log-Bayes Factors > 0). Bayes Factors larger than 1 present evidence in favor of the *rational integration* model. We evaluated the distribution of Bayes Factors following the classification of @lee2014bayesian.

## Results

```{r}
model_comp <- readRDS("../model/output/model_comparison.rds")

logbf <- model_comp %>%
  group_by(model)%>%
  summarise(margllh = logSumExp(loglike))%>%
  pivot_wider(names_from = model, values_from = margllh)%>%
  summarise(bf_comb_flat = combination - flatPrior, 
            bf_comb_prior = combination - priorOnly)
```

On a group-level, the results of the present study replicated those of @bohn2021young. The predictions made by the rational integration model were highly correlated with children’s responses in the combination task. The model explained around 74% of the variance in the data and with that more compared to the two alternative models (Figure \@ref(fig:fig3)A). Bayes Factors computed via the marginal likelihood of the data (Figure \@ref(fig:fig3)B) strongly favored the *rational integration* model in comparison to the *no common ground* ($BF_{10}$ = `r format(exp(logbf%>%pull(bf_comb_flat)),digits = 2)`) as well as the *no speaker informativeness* model ($BF_{10}$ = `r format(exp(logbf%>%pull(bf_comb_prior)),digits = 2)`).

```{r fig3, out.width="100%", fig.cap = "Group-level model comparison. A: Correlation between model predictions and data (aggregated across individuals and binned by year with 95\\%HDI) for each trial in the combination experiment. B: log-likelihood for each model given the data."}
knitr::include_graphics("./figures/fig3.png")
```

```{r}
mpred <- readRDS("../analysis/saves/id_model_pred.rds")

# note that the following code gives a different result each time because of the coin flip. To reproduce the numbers in the paper, please load the file form the saves folder. 

# chance_id <- mpred%>%
#   left_join(data %>% select(condition,subage,familiar,subid, correct))%>%
#   filter(!is.na(subage))%>%
#   #ungroup()%>%
#   rowwise()%>%
#   mutate(pred = flip(model_mean))%>%
#   ungroup()%>%
#   mutate(#match = ifelse(round(model_mean) == correct, 1, 0),
#          match = ifelse(pred == correct, 1, 0))%>%
#   group_by(model, subid)%>%
#   summarise(mean = mean(match, na.rm = T))%>%
#   summarise(correct = list(mean)) %>%
#   group_by(model)%>%
#   mutate(mean= mean(unlist(correct)),
#          sd = sd(unlist(correct)),
#          bf = extractBF(ttestBF(unlist(correct), mu = 1/2))$bf)
# 
# saveRDS(chance_id, "../analysis/saves/chance_id.rds")

chance_id <- readRDS("../analysis/saves/chance_id.rds")

id_bf <- readRDS("../analysis/saves/id_bf.rds")%>%
  left_join(data%>%select(subid,subage, age)%>%rename(id = subid)%>%distinct(id, .keep_all = T))

id_bf_comp <- id_bf%>%
  group_by(Comparison)%>%
  summarise(prop_pos_BF = sum(log_BF > 0)/60,
            prop_BF_3 = sum(log_BF > 1)/60,
            prop_BF_10 = sum(log_BF > 2.3)/60,
            prop_BF_neg3 = sum(log_BF < -1)/60,
            prop_BF_neg10 = sum(log_BF < -2.3)/60)
```

Next, we turned to the individual-level results. When looking at the proportion of correct predictions (for one run of the coin-flipping procedure), we saw that the *rational integration* model correctly predicted children’s responses in the combination task in `r round(chance_id%>%filter(model == "combination")%>%pull(mean),2)*100`% of trials, which was well above chance ($BF_{10}$ = `r format(chance_id%>%filter(model == "combination")%>%pull(bf),digits = 3)`) and numerically higher compared to the two alternative models (Figure \@ref(fig:fig4)A). Note that the alternative models also predicted children’s responses at a level above chance (*no common ground*: `r round(chance_id%>%filter(model == "flatPrior")%>%pull(mean),2)*100`%, $BF_{10}$ = `r format(chance_id%>%filter(model == "flatPrior")%>%pull(bf),digits = 3)`; *no speaker informativeness*: `r round(chance_id%>%filter(model == "priorOnly")%>%pull(mean),2)*100`%, $BF_{10}$ = `r format(chance_id%>%filter(model == "priorOnly")%>%pull(bf),digits = 3)`), emphasizing that they constitute plausible alternatives. In the supplementary material we also compared models with respect to the situations in which they did or did not correctly predict children's responses.

When directly comparing the models on an individual level, we found that the *rational integration* model provided the best fit for the majority of children. In comparison to the *no common ground* model, `r round(id_bf_comp%>%filter(Comparison == "bf_comb_flat")%>%pull(prop_pos_BF),2)*100`% of Bayes Factors were larger than 1 and `r round(id_bf_comp%>%filter(Comparison == "bf_comb_flat")%>%pull(prop_BF_10),2)*100`% were larger than 10. In comparison to the *no speaker informativeness* model, `r round(id_bf_comp%>%filter(Comparison == "bf_comb_prior")%>%pull(prop_pos_BF),2)*100`% of Bayes Factors were larger than 1 and `r round(id_bf_comp%>%filter(Comparison == "bf_comb_prior")%>%pull(prop_BF_10),2)*100`% were larger than 10 (Figure \@ref(fig:fig4)B).

```{r fig4, out.width="100%", fig.cap = "Individual-level model comparison. A: proportion of correct predictions for each model. Solid colored dots show mean with 95\\%CI for one run of the coin flip procedure. Light dots show aggregated individual data for the same run. Violins show distribution of means for 1000 runs of the procedure. B: distribution of log-Bayes Factors for each individual. Dashed lines show Bayes Factor thresholds of 3, 10 and 100."}
knitr::include_graphics("./figures/fig4.png")
```

## Discussion

The results of Part 2 show that the *rational integration* model accurately predicted children’s responses in the combination task. Importantly, this was the case not just on a group level, but also on an individual level where the model correctly predicted children’s responses in the majority of trials. Furthermore, it was more likely to be correct and provided a better explanation of the data compared to two alternative models that assumed that children selectively ignored some of the information sources.

# General discussion

Probabilistic models of cognition are often used to describe human performance in the aggregate, but these successes do not necessarily imply that they correctly describe individuals’ judgments. Instead, individual judgments could be produced via the operation of simpler heuristics. We investigated this study using rational speech act models of children’s pragmatic reasoning as a case study, using a computational cognitive model to make out-of-sample predictions about individual children’s behavior on a trial-by-trial basis. In Part 1, we used data from four tasks to estimate child-specific sensitivity parameters capturing their semantic knowledge, expectations about speaker informativeness, and sensitivity to common ground. In Part 2, we used these parameters to predict how the same children should behave in a new task in which all three information sources were jointly manipulated. We found strong support for our focal *rational integration* model in that this model accurately predicted children’s responses in the majority of trials and provided a better fit to individuals’ performance compared to two alternative heuristic models. Taken together, this work provides a strong test of the theoretical assumptions built into the model and both replicates and extends prior research that showed pragmatic cue integration in children’s word learning in the aggregate [@bohn2021young].

The *rational integration* model was built around three main theoretical assumptions. First, it assumes that children integrate all available information sources. The model comparison, in which we compared the focal model to two models that selectively ignore some of the information sources, strongly supported this assumption. For the majority of individuals -- as well as on a group level -- this model provided the best fit. However, for some individuals, one of the alternative models provided a better fit. Many of the Bayes Factors in these cases were relatively close to zero, but in a few cases, there was substantial evidence for the alternative models. Finding out why this is the case and what characterizes these individuals (e.g. if support for a lesioned model can be linked to other psychological constructs like attention or memory abilities) would be an interesting avenue for future research. Second, the model assumes that the integration process does not change with age. We did not probe this assumption in the present study because, in order to do so on an individual level, it would require longitudinal data -- an interesting extension for future work. Finally, the model assumes that children differ in their sensitivity to the different information sources but *not* in the way they integrate information. Even though a model using this assumption predicted the data well, it would also be interesting to explore structural differences between individuals [e.g. @franke2016reasoning].

Although our model explains and predicts data, we should be careful with granting the processes and parameters in it too much psychological realism. Nevertheless, we think that when studying individual differences, the model parameters can be interpreted as candidate latent measures of the psychological processes -- this interpretation is not necessarily worse than using  raw performance scores as a description of individuals [@borsboom2006attack].

In further support of the idea that model parameters can capture individual variation, our model parameters are estimated by taking into account the structure and the different processes involved in the task. This estimation process means that individual parameters can be based on data from multiple tasks, as, for example, semantic knowledge was estimated based on the mutual exclusivity, comprehension and production tasks. Support for such an approach comes from a recent study that used an RSA-type model estimate a single parameter capturing children’s pragmatic abilities based on data from three tasks [@bohn2022individual]. Taken together we think that computational modeling can make an important contribution to studying individual differences on a process level.

Our study is limited in terms of generalizability because we tested only one sample of children growing up in a western, affluent setting. However, the modeling approach put forward here provides an interesting way of studying and theorizing about cross-cultural differences. Following @bohn2019pervasive, our prima facie assumption is that children from different cultural settings might differ in terms of their sensitivity to different information sources -- just like individuals differ within cultural settings -- but the way that information is integrated is hypothesized to be the same across cultures. This prediction could be tested by comparing alternative models that make different assumptions about the integration process.

In sum, we have shown that children’s pragmatic word learning can be predicted on a trial-by-trial basis by a computational cognitive model. Together with previous work that focused on aggregated developmental trajectories [@bohn2021young], these findings suggest that the same computational processes  -- a pragmatic inference process that integrates sources of information in a rational manner -- can be used to predict group- and individual-level data. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
