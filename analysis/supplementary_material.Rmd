---
title: "Modeling individual differences in children's information integration during pragmatic word learning"
subtitle: "Supplementary material"
#author: "Manuel Bohn, Louisa S. Schmidt, Cornelia Schulze, Michael C. Frank and Michael Henry Tessler"
output: 
  bookdown::pdf_document2:
    toc: yes 
    number_sections: false    
  bookdown::html_document2:
    toc: yes
    toc_float: true
    fig_caption: yes
    #code_folding: hide
    number_sections: false
bibliography: ../../../References/library.bib
#csl: pnas.csl
#header-includes:
#  \usepackage{caption}
#  \renewcommand{\figurename}{Supplementary Figure}
#  \renewcommand{\tablename}{Supplementary Table}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, size="small")

library(tidyverse)
library(ggthemes)
library(jsonlite)
library(readxl)
library(tidyboot)
library(matrixStats)
#library(GGally)
library(coda)
library(data.table)
library(ggpubr)
library(ggridges)
library(knitr)
library(BayesFactor)


estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}


flip <- function(x){
  coin <- sample(c(0,1), size = 1, replace = TRUE, prob = c(1 - x,x))
  return(coin)
}

```

# Overview

The goal of the study was to predict information integration during pragmatic word learning in children on a trial-by-trial basis. We measured children's sensitivity to different information sources and then used an RSA model to generate how the same children should behave when these information sources need to be integrated. 

# Empirical studies

```{r}
data <- read_csv("../data/merged_data.csv")
```

## Sample size

Table \@ref(tab:parttab) gives an overview of the participants. All 60 participants participated in all tasks (4 tasks to measure sensitivity to information sources and the combination task).

```{r parttab}
t1 <- data%>%
  group_by(subage, sex)%>%
  summarise(n = length(unique(subid)))%>%
  rename("Age group" = subage,
         "Sex" = sex, 
         "N" = n)

knitr::kable(t1, caption = "Sample size and demographic information.", digits = 2, align = "l")
```

## Sensitivity experiments

### Setup

The different tasks were programmed as interactive picture books in `JavaScript/HTML` and presented on a website. During the video call (via BigBlueButton), participants would enter the website with the different tasks and share their screen. The experimenter guided them through the procedure and told caregivers when to advance to the next task. Children responded by pointing to objects on the screen, which their caregivers would then select for them via mouse click. For the production task, the experimenter shared their screen and presented pictures in a slide show. For the mutual exclusivity, discourse novelty, and combination tasks, pre-recorded sound files were used to address the child.

### Tasks

We used the same tasks as in @bohn2021young to measure children's sensitivity to the different information sources, that is:
 
* Mutual Exclusivity
* Discourse novelty

In addition, we used two new vocabulary tasks:

* Word production
* Word comprehension

Across all tasks (except discourse novelty) we used a total of 16 familiar objects. There was one trial for each familiar object in mutual exclusivity, word production and comprehension. For the 16 familiar objects, the level of familiarity varied. Four objects (duck, bread, carrot, apple) were retained from @bohn2021young and can be considered highly familiar objects for 3- and 4-year-old children. Twelve additional objects were modeled after a previous mutual exclusivity study involving German 3- and 4-year-olds @grassmann2015children. Some of these objects (e.g., corkscrew) are barely familiar for children in this age group. 

#### Training 

Generally, each child participated in two sessions. Both sessions began with a brief training. First, the child was instructed to point to colorful dots on the screen, which disappeared when their parent selected them. This way the child could practice pointing at the screen and the parent could practice interpreting their child's pointing gestures. Next, there were two training trials where animal speakers requested highly familiar objects (duck and teddy bear) from the child. This introduced children to the procedure of the following tasks, where animals requested objects from them and they had to point to the requested object. 

#### Mutual exclusivity experiment 

Each child participated in the mutual exclusivity experiment (16 trials) on the first testing day. Here, animal speakers stood on a small hill between two tables on which objects were placed. One of these objects was potentially familiar to the child, the other object was unknown. After introducing themselves, speakers requested objects from the child using novel labels label (e.g., “Oh, cool, da liegt ein Höfas auf dem Tisch, wie toll! Ein Höfas liegt auf dem Tisch! Kannst du mir das Höfas geben?” [“Oh, cool, there is a höfas on the table, how neat! A höfas is on the table. Can you give me the höfas?”]). 

The mutual exclusivity experiment had only one condition. Each familiar and unknown object appeared once; each speaker appeared four times. The dependent variable was the object the child selected on each trial (i.e., choice). Here, choosing the unknown object constituted the correct choice and was coded as 1. 

#### Discourse novelty experiment 

Each child participated in the discourse novelty experiment (12 trials) on the first testing day, immediately following the mutual exclusivity experiment. Animal speakers again stood on a small hill between two tables. Contrary to the mutual exclusivity experiment, in the beginning, only one of the tables contained an object, while the other was empty. Speakers commented on the presence of the object (“Aha, schau mal da!” [“Aha, look at that!”]) and the absence of an object on the other table (“Hm, da ist nichts!” [“Hm, nothing there!”]). These utterances were designed to create common ground between the speaker and the child. Both the side where the empty table stood (left or right) and which of these comments the speaker uttered first was counterbalanced across trials. Speakers disappeared from the screen after the sound of a ringing telephone could be heard. (The child had been told beforehand that at times, the animals would have to leave to answer calls.) During the speakers' absence, a second object appeared on the previously empty table. Shortly after, the speaker returned and requested an object in the same manner as in the mutual exclusivity experiment (e.g., “Oh, toll, da liegt ein Wisslo auf dem Tisch, wie interessant! Ein Wisslo liegt auf dem Tisch! Kannst du mir das Wisslo geben?” [“Oh, neat, there’s a wisslo on the table, how interesting! A wisslo is on the table. Can you give me the wisslo?”]).  

The discourse novelty experiment had only one condition. Each unknown object appeared once; each speaker appeared three times. Again, the dependent variable was choice. Here, choosing the object that appeared later during the trial and was thus new in the discourse context constituted the correct choice and was coded as 1. 
  
#### Combination experiment 

Each child participated in the combination experiment (16 trials) on the second testing day. Here, mutual exclusivity and discourse novelty were combined. Overall, the procedure was the same as in the discourse novelty experiment. However, in contrast to the discourse novelty experiment, only one of the object was unknown, while the other object was familiar. Speakers requested objects in the same manner as before. 

The combination experiment had two conditions. In the congruent condition, the object that was new in the discourse context was an unknown object, that is, the pragmatic cues mutual exclusivity and discourse novelty were aligned. In the incongruent condition, the object that was new in the discourse context was a familiar object, that is, the pragmatic cues were disaligned. 

Objects with comparable familiarity were assigned to the conditions (e.g., children are similarly familiar with duck and carrot; duck was assigned to the congruent condition and carrot was assigned to the incongruent condition). The combination experiment consisted of 8 congruent and 8 incongruent trials (4 congruent and 4 incongruent trials when the procedure was stopped early). For our coding, choosing the unknown object constituted the correct choice and was coded as 1. 

For all three tasks, children received the same order of trials to ensure comparability across participants.

#### Reliability Coding

Reliability coding was performed to ensure that the objects parents selected matched the objects children had chosen via pointing in each respective trial. For a randomly selected 20% of the sample (n = 12), children’s object choices were coded from video by a coder who was blind to the data from parents’ selections. The results indicate that parents faithfully selected children’s chosen objects (99.74% match, $\kappa$= .997, *p* < .001). 

#### Word production task

The word production and the comprehension task were run on the second testing day after the mutual exclusivity, the discourse novelty and the combination experiment. 

For the production task, the experimenter showed the child an image of an object (the same as was used in the other experiments) and prompted them to label the object with probing questions such as: “Weißt du, wie man das hier nennt?” [“Do you know what this is called?”] or “Was ist das?” [“What’s this?”]. The trial was coded as correct and received a score of 1 if the child produced a label that was correct according to coding scheme that had been developed prior to data collection based on piloting data. Otherwise, the child's answer received a score of 0. The experimenter went through each of the 16 objects one by one. the coding scheme is available in the associated repository at `documentation/production_task_list_of_labels.pdf`.

The picture below shows an example.The images were the same as the ones used in the mutual exclusivity task and the comprehension task. 

```{r out.width="30%", fig.cap = "Example stimulus for the production task.", fig.align='center'}
knitr::include_graphics("../documentation/production_task_example_pic.png")
```

#### Comprehension task

The child was shown images of 6 objects on a screen, 4 of which were objects that appeared in the rest of the study and 2 were distractors. The experimenter asked the child to pick out an object (e.g., “Wo ist der Korkenzieher?” [“Where is the corkscrew?”] or “Kannst du auf das Schloss zeigen?” [“Can you point to the lock?”]). The child responded by pointing at an object, which the parent then selected. 

We coded as correct if the child selected the correct object OR if they had correctly named the object during production.

The experimenter went through all 16 familiar objects on a total of 4 slides (each showing 6 objects, including 2 distractors).

The picture below shows an example (familiar objects from the study: rasp, lock, apple, hanger; distractors: dog toy and hydrant).

```{r, out.width="80%", fig.cap = "Example for layout of a trial in the comprehension task.", fig.align='center'}
knitr::include_graphics("../documentation/comprehension_task_example_pic.png")
```

### Results

```{r, cache = T}
p1 <- data %>%
  filter(task != "combination")%>%
  group_by(subage,age,task, subid) %>%
  filter(!grepl("train",trial)) %>%
  summarise(mean = mean(correct, na.rm = T))%>%
  mutate(age = age+3)

p2 <- p1 %>%
  group_by(subage,task) %>%
  tidyboot_mean(column = mean, na.rm = T)%>%
  mutate(chance = ifelse(task == "mutual_exclusivity" | task == "novelty", 1/2,
                         ifelse(task == "comprehension", 1/6, NA)))

po <- ggplot()+
  geom_hline(data = p2, aes(yintercept =chance), lty = 2, alpha = .5)+
  geom_smooth(data = p1, aes(x = age, y = mean), method = "lm", col = "black", size = 1)+
  geom_point(data = p1, aes(x = age, y = mean), alpha = .25)+
  geom_pointrange(data = p2, aes(x = subage+.5, y = mean, ymin = ci_lower, ymax = ci_upper, col = factor(subage)), position = position_dodge(width = .5))+
  facet_wrap(~task, nrow = 2)+
  labs(x = "Age", y = "Proportion Correct") +
  scale_color_ptol(name = "Age group", labels = c("3-year-olds", "4-year-olds"))+
  ylim(-0.02, 1.02)+
  theme_few()
```

Figure \@ref(fig:exp)A shows the results for the four sensitivity experiments designed to measure children's sensitivity to individual information sources. In all cases, children of all ages children were sensitive to the respective information sources (i.e. performance above chance or above 0) For word comprehension and production and mutual exclusivity we see an increase of performance with age whereas for discourse novelty, performance was stable across development.

Furthermore, the three tasks measuring children's semantic knowledge were positively correlated (Figure \@ref(fig:exp)B). However, the correlation was not perfect, suggesting that the tasks were not redundant. 

```{r}
cpid1<- data%>%
  filter(task == "mutual_exclusivity" | task == "comprehension" | task == "production")%>%
  group_by(subid,subage, task)%>%
  summarise(mean = mean(correct, na.rm = T))%>%
  mutate(subage = ifelse(subage == 3, "3-year-olds", "4-year-olds"))

cpid <- data%>%
  filter(task == "mutual_exclusivity" | task == "comprehension" | task == "production")%>%
  group_by(subid,subage, task)%>%
  mutate(subage = ifelse(subage == 3, "3-year-olds", "4-year-olds"))%>%
  summarise(mean_task1 = mean(correct, na.rm = T))%>%
  left_join(cpid1 %>% rename(task2 = task, mean_task2 = mean),by = c("subid","subage"))%>%
  filter(task != task2)%>%
  rowwise()%>%
  mutate(ex = paste(sort(c(task, task2)), collapse = " - "))%>%
  distinct(ex, .keep_all = T)
  

skexp <- ggplot(cpid, aes(x = mean_task1, y = mean_task2, col = subage))+
  geom_count( alpha = .5)+
  geom_abline(intercept = 0 , slope = 1, lty = 2, alpha = .4)+
  facet_grid(task2~task)+
  theme_few()+
  geom_smooth(method = lm,inherit.aes = F, aes(x = mean_task1, y = mean_task2), col = "black", size = 0.75)+
  labs(x = "", y = "")+
  guides(size = F)+
  scale_color_ptol(name = "Age group") +
  stat_cor(method = "pearson",inherit.aes = F, aes(x = mean_task1, y = mean_task2),size = 3, r.accuracy = 0.01, p.accuracy = 0.01, cor.coef.name = "r")+
  scale_x_continuous(limits = c(-0.01,1.01),breaks = c(0,1))+
  scale_y_continuous(limits = c(-0.01,1.01),breaks = c(0,1))

```

```{r exp, fig.cap = "A) Results of individual experiments. Colored points show means (with 95\\% CI) for data binned by year, light black dots show participant means. Dotted line shows level of performance expected by chance. B) Correlation between the three tasks measuring semantic knowledge.",fig.height = 5, fig.width= 10, out.width="100%"}
ggarrange(po, skexp, common.legend = T, labels = c("A","B"), legend = "right")
```

## Combination experiment

There were also 16 trials in combination, half of which were in the congruent condition and the other half were in the incongruent condition. The familiar objects from the mutual exclusivity experiment were therefore split across the two conditions.

The combination of condition (congruent/incongruent) and familiar object was the same for all children and so was the order of trials and side counterbalancing.

### Results

Since there is no clear right or wrong answer it is difficult to evaluate the combination experiment on its own. Figure \@ref(fig:compexp) below gives some kind of sanity check, namely that in the congruent case, children should be more likely to choose the unfamiliar object compared to the mutual exclusivity experiment whereas in the incongruent condition they should be less likely to do so. This is generally what we see. 

```{r}
p_c <- data%>%
  filter(task == "mutual_exclusivity" | task == "combination")%>%
  group_by(subage,task,familiar)%>%
  summarise(correct = mean(correct, na.rm = T))%>%
  mutate(subage = ifelse(subage == 3, "3-year-olds", "4-year-olds"))%>%
  left_join(data%>%filter(task =="combination")%>%select(familiar,condition)%>%distinct())%>%
  ggplot(., aes(x = task, y = correct, col = familiar))+
  geom_line(aes(group = familiar))+
  geom_point(alpha = .75)+
  ylab("Proportion unfamiliar object chosen")+
  xlab("Task")+
  theme_few()+
  scale_color_discrete(name = "Familiar object")+
  ylim(0,1)+
  facet_grid(subage~condition)
```

```{r compexp, fig.cap = "Average proportion with which the unfamiliar object was chosen in the mututal exclusivity task and the combination experiment depending on the familiar object. Congruent and incongruent refers to the condition in which the familiar object appeared in the combination experiment.",fig.height = 6, fig.width= 10, out.width="100%"}
p_c
```
# Cognitive models

## Modelling framework

We adopted the modelling framework used by @bohn2021young. A detailed description of that framework can be found in their supplementary material (click [here](https://github.com/manuelbohn/spin/blob/master/paper/supplementary_material.pdf) for direct access to this document). In short, our models are situated in the Rational Speech Act (RSA) framework [@frank2012predicting; @goodman2016pragmatic]. RSA models are models of pragmatic reasoning and treat language understanding as a special case of Bayesian social reasoning. A listener interprets an utterance by assuming it was produced by a cooperative speaker who had the goal to be informative. Being informative is defined as providing messages that increase the probability of the listener inferring the speaker’s intended message. The *rational integration* model, including all data-analytic parameters, is formally defined as: 

\begin{equation}
P_{L_1}(r \mid u; \{\rho_i, \alpha_i\, \theta_{ij}\})\propto P_{S_1}(u \mid r; \{\alpha_i, \theta_{ij}\}) \cdot P(r \mid \rho_i)
(\#eq:rsafull1)
\end{equation}

\begin{equation}
P_{S_1}(u \mid r; \{\alpha_i\, \theta_{ij}\})\propto P_{L_0}(r \mid u; \{\theta_{ij}\}) ^{\alpha_i}
(\#eq:rsafull2)
\end{equation}

\begin{equation}
P_{L_0}(r \mid u; \{\theta_{ij}\}) \propto \mathcal{L}(u, r \mid \theta_{ij})
(\#eq:rsafull3)
\end{equation}

Thus, the model describes a listener ($L_1$) reasoning about the intended referent of a speaker's ($S_1$) utterance. This reasoning is contextualized by the prior probability of each referent $P(r \mid \rho_i)$. This prior probability is a function of the common ground $\rho$ shared between speaker and listener in that interacting around the objects changes the probability that they will be referred to later. We assume that children differ in their sensitivity to common ground and each child $i$ is therefore represented by a different parameter value. 

To decide between referents, the listener ($L_1$) reasons about what a rational speaker ($S_1$) would say given an intended referent. This speaker is assumed to compute the informativity for each available utterance and then choose the most informative one. The expectation of speaker informativeness may vary and is captured by the parameter $\alpha$. Again, we assume that children differ in their expectations about how informative the speaker and each child $i$ is therefore represented by a different value of $\alpha$. 

The informativity of each utterance is given by imagining which referent a literal listener ($L_0$), who interprets words according to their lexicon $\mathcal{L}$, would infer upon hearing the utterance. This reasoning depends on what kind of semantic knowledge (word--object mappings) the speaker thinks the literal listener has. We parameterize the listener's knowledge of a word's semantics in terms of a semantic knowledge parameter $\theta$, which varies between 0 and 1. $\theta = 0$ corresponds to the state of knowledge for a completely novel word and results in a semantic interpretation function that chooses randomly between the objects in the scene. For each of the novel words, the literal listener is assumed to have semantic knowledge of 0. For $\theta \in (0, 1)$, the semantic interpretation function will select the familiar referent with probability $\theta + (1 - \theta)\frac{1}{2} = \frac{1 + \theta}{2}$; that is, with probability $\theta$, the listener knows the correct meaning of the word (and picks out the correct referent 100\% of the time); with probability $1 - \theta$, the listener does not know the meaning of the word and must guess, picking out the correct referent 50\% of the time. For familiar objects, we take semantic knowledge to be a function of the degree-of-acquisition of the associated word. We assume that children differ in their semantic knowledge fot the different familiar objects and $\theta$ therefore depends on the word $j$ and the child $i$.

## Model parameters

As mentioned above, each child in our model is represented by their own set of data-analytic parameters ($\rho_i$,$\alpha_i$, and $\theta_{ij}$). This will allow us to later use the model to generate participant-specific model predictions for the integration experiment. We estimated these parameters based on four  sensitivity experiments. All parameters were estimated via hierarchical regression (mixed-effects) models. For each parameter, we estimated an intercept and slope (fixed effects) that best described the developmental trajectory for this parameter based on the available data. Participant-specific parameters values (random effects) were estimated as deviations from the value expected for a participant based on their age.   

### Semantic knowledge

The parameters for semantic knowledge were simultaneously informed by the data from the mutual exclusivity, the comprehension and the production experiments. To leverage the mutual exclusivity data, we adopted the RSA model described above to a situation in which both objects (novel and familiar) had equal prior probability (i.e., no common ground information). In the same model, we also estimated the parameter for speaker informativeness (see below). For the comprehension experiment, we assumed that the child knew the referent for the word with probability $\theta_{ij}$. If $\theta_{ij}$ indicated that they knew the referent (a coin with weight $\theta_{ij}$ comes up heads) they would select the correct picture; if not they would select the correct picture at a rate expected by chance (1/6). Likewise, for the production experiment, we assumed that the child knew the word for the referent with probability $\theta_{ij}$. If $\theta_{ij}$ indicated that they knew the word (a coin with weight $\theta_{ij}$ comes up heads), we assumed the child would be able to  produce it with probability $\gamma$. This successful-production-probability $\gamma$ was the same for all children and was inferred based on the data. This adjustment reflects the finding that children's receptive vocabulary is larger than the productive. Taken together, for each child $i$ and familiar object $j$ there were three data points to inform $\theta$: one trial from the mutual exclusivity, one from the comprehension and one from the production experiment.

The participant- and object-specific parameter ($\theta_{ij}$) was estimated in the form of a hierarchical regression model: $\theta_{ij} = \text{logistic}(\beta^{\theta}_{0, j} + i \cdot \beta^{\theta}_{1, j})$; each word's lexical development trajectory (the intercept $\beta^{\theta}_{0, j}$  and slope $\beta^{\theta}_{1, j}$ of the regression line for each object) was estimated as a deviation from an overall trajectory of vocabulary development. The intercept and slope for each item were sampled from Gaussian distributions with means $\mu^\theta_0, \mu^\theta_1$ and variances $\sigma^\theta_0, \sigma^\theta_1$: $\beta^{\theta}_{0, j} \sim \mathcal{N}(\mu^\theta_0, \sigma^\theta_0)$ and $\beta^{\theta}_{1, j} \sim \mathcal{N}(\mu^\theta_1, \sigma^\theta_1)$. $\mu^\theta_0$ and $\mu^\theta_1$ represented the overall vocabulary development independent of particular familiar word--object pairings, and $\sigma^\theta_0$ and $\sigma^\theta_1$ represented the overall variability of intercepts and of slopes between items (see Figure \@ref(fig:bda)). 

In this model, the participant-specific value for $\theta$ was generated via $i$. Given the structure of the model, it is natural to think of $i$ as the child's age. However, we took $i$ to be the child's "linguistic" age. That is, we assumed that a child's semantic knowledge might be higher (or lower) than what we would expect given the child's numerical age. Thus, we sampled $i$ from a Gaussian distribution with mean $k$ and variance $\sigma^\theta_i$: $i \sim \mathcal{N}(k, \sigma^\theta_i)$. Here $k$ was the child's numerical age and $\sigma^\theta_i$ the overall variability in linguistic age between children. This procedure allowed us to inform the participant specific value for $\theta$ both by the child's responses but also by the overall developmental trajectory in the data.

### Expectations about speaker informativeness

The parameter representing a child's expectations about how informative speakers are, was estimated based on the data from the mutual exclusivity experiment. As mentioned above, this was done jointly with semantic knowledge in a RSA model adopted to a situation with equal prior probability of the two objects (novel and familiar).

To estimate the participant specific parameter, we used the same approach as for semantic knowledge. That is, $\alpha_i$ was estimated via a linear regression -- $\alpha_i = \beta^\alpha_0 + i \cdot \beta^\alpha_1$ -- in which $\beta^\alpha_0$ and $\beta^\alpha_1$ defined a general developmental trajectory. Again, we assumed that children might deviate from their expectations about speaker informativeness based on their numerical age and so we estimated $i$ as a deviation from the child's numerical age $k$: $i \sim \mathcal{N}(k, \sigma^\alpha_i)$.

### Sensitivity to common ground

We estimated children's sensitivity to common ground based on the data from the discourse novelty experiment. We used a logisitc regression model to estimate the average developmental trajectory: $\rho_i = \text{logistic}(\beta^\rho_0 + i \cdot \beta^\rho_1)$. To generate participant specific values for $\rho$ we again estimated $i$ as a deviation from the child's numerical age $k$: $i \sim \mathcal{N}(k, \sigma^\rho_i)$.  

### Parameter estimation

All cognitive models and regression models were implemented in the probabilistic programming language `WebPPL` [@dippl]. The corresponding model code can be found in the associated online repository (file `model/spin-within_model_prediction.wppl`). We used the following prior distributions for model parameters. Intercept and slope for sensitivity to common ground: $\beta^\rho_0 ,\beta^\rho_1 \sim \text{Uniform}(-2,2)$. Variation between participants: $\sigma^\rho_i \sim \text{Uniform}(0,4)$.

Speaker informativeness:  $\beta^\alpha_0 \sim \text{Uniform}(-3,3)$ for the intercept and $\beta^\alpha_1 \sim \text{Uniform}(-0,4)$ for the slope. We restricted the slope to be positive because negative values for speaker informativeness are conceptually implausible. We also did not expect sensitivity to speaker informativeness to decrease across our age range. For variation between participants we used: $\sigma^\alpha_i \sim \text{Uniform}(0,2)$.

For the global semantic knowledge parameters, we used $\mu^\theta_0 \sim \text{Uniform}(-3,3)$ for the intercept and $\mu^\theta_1 \sim \text{Uniform}(0,2)$ for the slope, because it is implausible to assume that semantic knowledge decreases with age. For the parameters capturing the variability the object specific trajectories around these overall parameters we used $\sigma^\theta_0, \sigma^\theta_1 \sim \text{Uniform}(0,2)$. Variation between participants: $\sigma^\theta_i \sim \text{Uniform}(0,2)$.

To estimate the parameter distributions, we collected samples from six independent MCMC chains, collecting 750,000 samples from each chain and removing the first 250,000 for burn-in. Of the 500,00 remaining samples, we used every 10th to construct the posterior distribution. We excluded samples from two chains because they got stuck on a local maximum which resulted in parameter distributions that were substantially different from the other chains. The [Appendix](#appendix-2-model-parameters) visualizes some examples of these distributions.

## Model predicitons

We used the posterior distributions for the three parameters to generate a-priori model predictions for how the same participants should behave in the combination experiment. That is, we passed the parameter distributions estimated based on the four separate sensitivity experiments through our rational integration model to generate  predictions for how children should behave in the combination experiment. Because our model parameters were specific to each child and familiar object, our model predictions were also specific to each trial in the combination experiment (i.e. unique combination of participant, familiar object and condition).

Remember that there were two conditions in the combination experiment, depending on how the mutual exclusivity and the common ground manipulations were aligned. In the *congruent* condition, the object in the prior social interaction (i.e., the discourse-familiar object) was the familiar object, and hence, both common ground (discourse novelty) and mutual exclusivity are cues to the same referent; in the *incongruent* condition, the object in the prior social interaction was the novel object, and hence, common ground (discourse novelty) and mutual exclusivity are cues to different referents.

## Model evaluation

To evaluate the model predictions, we first replicated the group--level results of @bohn2021young. The goal was to show that a) model predictions are highly correlated with the data and b) the *rational integration* model makes better predictions compared to a set of alternative models. Next, we turned to the focal participant--level analysis and evaluated how well the *rational integration* model predicted the trial-by-trail performance of each child, both on an absolute level and in comparison to the alternative models. 

### Alternative models

In addition to the *rational integration* model, we considered two alternative, lesioned models. Both models represent thy hypothesis that children focus on one type of inference (mutual exclusivity or common ground) whenever multiple inferences are possible. The predictions of these alternative models were generated using the same parameters as the *rational integration* model, the difference lies in which parameters were used. 

The *no speaker informativeness* model assumed that the speaker does not communicate in an informative way. This corresponds to $\alpha_i$ = 0, which causes the likelihood term to always be 1. As a consequence, this model also ignores semantic knowledge (which affects the likelihood term) and the predictions of this model correspond to the prior distribution over objects: 

\begin{equation}
P^{no\_si}_{L_1}(r \mid u; \{\rho_i\})\propto P(r \mid \rho_i) 
(\#eq:nome)
\end{equation}

On the other hand, the *no common ground* model ignores common ground information, $\rho_i$. This model takes in object specific semantic knowledge and speaker informativeness but uses a prior distribution over objects that is constant across alignment conditions and uniform (e.g., [0.5, 0.5]). This model corresponds to a listener who only focuses on the mutual exclusivity inference and ignores the common ground manipulation. As a consequence, the listener does not differentiate between the two common ground alignment conditions.

\begin{equation}
P^{no\_cg}_{L_1}(r \mid u; \{\alpha_i\, \theta_{ij}\})\propto P_{S_1}(u \mid r; \{\alpha_i, \theta_{ij}\}) \\ 
(\#eq:nocg)
\end{equation}

### Correlation between model predictions and data

```{r, cache=T}
data_binned <- data%>%
  filter(task == "combination")%>%
  group_by(subage, condition,familiar)%>%
  summarize(k = sum(correct, na.rm = T), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         data_mean = (a-1)/(a+b-2),
         data_ci_lower  = qbeta(.025, a, b),
         data_ci_upper = qbeta(.975, a, b),
         subage = factor(subage)
         )%>%
  select(-a,-b,-n,-k)#%>%
# 
# 
# model_binned_comb <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "combination",
#          chain != 1, 
#          chain != 2)%>%
#   left_join(data %>% select(condition,familiar,subid,subage))%>%
#   filter(!is.na(subid), !is.na(subage))%>%
#   mutate(subage = factor(subage))%>%
#   group_by(model,subage, condition, familiar)%>%
#   summarise(model_mean = mean(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# model_binned_flat <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "flatPrior",
#          chain != 1, 
#          chain != 2)%>%
#   left_join(data %>% select(condition,familiar,subid,subage))%>%
#   filter(!is.na(subid), !is.na(subage))%>%
#   mutate(subage = factor(subage))%>%
#   group_by(model,subage, condition, familiar)%>%
#   summarise(model_mean = mean(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# model_binned_prior <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "priorOnly",
#          chain != 1, 
#          chain != 2)%>%
#   left_join(data %>% select(condition,familiar,subid,subage))%>%
#   filter(!is.na(subid), !is.na(subage))%>%
#   mutate(subage = factor(subage))%>%
#   group_by(model,subage, condition, familiar)%>%
#   summarise(model_mean = mean(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# model_binned <- bind_rows(
#   model_binned_comb,
#   model_binned_flat,
#   model_binned_prior
# )
# 
# saveRDS(model_binned, "saves/cor_model_pred.rds")

model_binned <- readRDS("saves/cor_model_pred.rds")

cor_plot <-  bind_rows(model_binned %>% left_join(data_binned)
                       )%>%
  mutate(subage = ifelse(subage == 3, "3-year-olds", "4-year-olds"),
         model = factor(model, levels = c("combination", "flatPrior","priorOnly"),labels=c("Integration","No Common Ground","No Speaker Informativeness")))

cor_plot1 <- cor_plot%>%
  filter(model == "Integration")%>%
ggplot(aes(x = model_mean, y = data_mean, col = subage)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .5, alpha = .7)+
  geom_errorbarh(aes(xmin = model_ci_lower, xmax = model_ci_upper), height = 0,size = .5, alpha = .7)+
  geom_point(size = 1.5, stroke = 1, pch = 5)+
  coord_fixed()+
  facet_grid(~model)+
stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = model_mean, y = data_mean, label = paste(..rr.label..)), inherit.aes = F, size = 3)+
  xlab("Model predictions")+
  ylab("Child inference data\n(proportion novel object chosen)")+
  theme_few() + 
  scale_colour_ptol(name = "Age group")+
  scale_x_continuous(limits = c(0,1), breaks = c(0,1))+
  scale_y_continuous(limits = c(0,1), breaks = c(0,1))

cor_plot2 <- cor_plot%>%
  filter(model != "Integration")%>%
ggplot(aes(x = model_mean, y = data_mean, col = subage)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .5, alpha = .7)+
  geom_errorbarh(aes(xmin = model_ci_lower, xmax = model_ci_upper), height = 0,size = .5, alpha = .7)+
  geom_point(size = 1.5, stroke = 1, pch = 5)+
  coord_fixed()+
  facet_grid(model~.)+
stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = model_mean, y = data_mean, label = paste(..rr.label..)), inherit.aes = F, size = 3)+
  xlab("")+
  ylab("")+
  theme_few() + 
  scale_colour_ptol(name = "Age group")+
  scale_x_continuous(limits = c(0,1), breaks = c(0,1))+
  scale_y_continuous(limits = c(0,1), breaks = c(0,1))

```

```{r corplot, fig.cap = "Schematic experimental procedure with screenshots from the experiments.",fig.height = 4, fig.width= 10, out.width="100%"}
ggarrange(cor_plot1, cor_plot2, common.legend = T, legend = "right", widths = c(1.5,1))
```

As a first step, we correlated the model predictions with the data. To do so, we aggregated both the model predictions and the data for each trial (each with a different familiar object, half in the congruent and the other half in the incongruent condition) within each age group. Figure \@ref(fig:corplot) shows a strong correlation between model predictions and the data with the model explaining 74% of the variance int he data. This correlation was comparable in size to that in @bohn2021young. The correlation was also higher for the *rational integration* compared to the lesioned models.

```{r}
#ggsave("../graphs/comb_cor.png", width = 6, height = 4, scale = 1)
```

### Pairwise model comparisons

```{r}
model_comp <- readRDS("../model/output/model_comparison.rds")
```

```{r}
logbf <- model_comp %>%
  group_by(model)%>%
  summarise(margllh = logSumExp(loglike))%>%
  pivot_wider(names_from = model, values_from = margllh)%>%
  summarise(bf_comb_flat = combination - flatPrior, 
            bf_comb_prior = combination - priorOnly)
```


```{r}
lp <- model_comp %>%
  group_by(model)%>%
  summarise(margllh = logSumExp(loglike))%>%
  ggplot(aes(x = model, y = margllh, col = model))+
  geom_segment( aes(x=model, xend=model, y=margllh, yend=-400), col = "black", size = 1)+
  geom_point(size = 7)+
  theme_few()+ 
  xlab("")+
  ylab("Log-likelihood")+
  theme(axis.text.x=element_text(angle = 45, vjust = 1, hjust = 1))+
  guides()+
  ylim(-650,-400)+
  scale_color_manual(name = "Model",
                     labels=c("Integration","No Common Ground","No Speaker Informativeness"),
                     
                    values= c("#6a994e","#6e1423","#a71e34"))+
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
```

```{r loglikeplot, fig.cap = "Schematic experimental procedure with screenshots from the experiments.",fig.height = 2.5, fig.width= 10, out.width="100%"}
lp
```

Furthermore, we compared the three models via the marginal likelihood of the data. Here we also saw that the *rational integration* model fit the data best (Figure \@ref(fig:loglikeplot)). For pairwise comparisons, we used the marginal likelihoods to compute Bayes factors in favor of the *rational integration* model. There was overwhelming evidence that the *rational integration* model outperformed the *no common ground* ($BF_{10}$ = `r format(exp(logbf%>%pull(bf_comb_flat)), digits = 2)`) and the *no speaker informativeness* model ($BF_{10}$ = `r format(exp(logbf%>%pull(bf_comb_prior)), digits = 2)`).

## Participant-specific model predictions

The results in the last section showed that the *rational integration* model made accurately predicted the average performance in each age group. However, the main focus of the present study was to see if this model also made accurate predictions on an individual level. That is, we wanted to see how well we could predict the behavior of individual children on individual trials.


```{r}
# model <- rbindlist(
#   list(
#   fread("../model/output/spin-within_model_slope_variance_prediction-50000_burn250000_lag9_chain1.csv"),
#   fread("../model/output/spin-within_model_slope_variance_prediction-50000_burn250000_lag9_chain2.csv"),
#   fread("../model/output/spin-within_model_slope_variance_prediction-50000_burn250000_lag9_chain3.csv"),
#   fread("../model/output/spin-within_model_slope_variance_prediction-50000_burn250000_lag9_chain4.csv"),
#   fread("../model/output/spin-within_model_slope_variance_prediction-50000_burn250000_lag9_chain5.csv"),
#   fread("../model/output/spin-within_model_slope_variance_prediction-50000_burn250000_lag9_chain6.csv")
# ), use.names=TRUE)

# model %>%filter(a == "prediction")%>%saveRDS("../model/output/comb_model_predictions.rds")
# 
# model %>% filter(a != "prediction")%>%saveRDS("../model/output/comb_model_parameters.rds")
```


```{r}
#model_predictions <- readRDS("../model/output/comb_model_predictions.rds")
# model_parameters <- readRDS("../model/output/comb_model_parameters.rds")
```

The participant and familiar object specific parameters allowed us to generate predictions for every single trial for each participant. These predictions were generated via the parameters estimated based on the sensitivity experiments. For each sample of the posterior distribution of these parameters, we generated a prediction for the combination experiment. As a consequence, there was not a single prediction for each trial, but a distribution of predictions. The data we collected, however, was binary because for each trial we coded whether the child picked the unfamiliar object or not. Thus, to evaluate how well the model predicted the child's behavior on a given trial, we had to convert the distribution of continuous model predictions into a binary prediction. To do so, we first computed the Maximum-a-posteriori (MAP) of the predictive distribution for each child and trial and then flipped a coin with the MAP as its weight. Figure \@ref(fig:genfig) gives a schematic overview of this process.

```{r genfig, out.width="100%", fig.cap = "Schematic overview of how participant and trial specific model predictions were generated."}
knitr::include_graphics("../graphs/model_pred.png")
```

We then compared the model's prediction for a given trial to the data simply by asking whether the two matched (coded as `1`) or not (coded as `0`). The number of matches (correct predictions) was then evaluated in three ways: 

```{r}

# bf_chance <- tibble()
# 
# for (i in 1:1000) {
#   
#   x <- mpred%>%
#   left_join(data %>% select(condition,subage,familiar,subid, correct))%>%
#   filter(!is.na(subage))%>%
#   #ungroup()%>%
#   rowwise()%>%
#   mutate(pred = flip(model_mean))%>%
#   ungroup()%>%
#   mutate(#match = ifelse(round(model_mean) == correct, 1, 0),
#          match = ifelse(pred == correct, 1, 0))%>%
#   group_by(model, subid)%>%
#   summarise(mean = mean(match, na.rm = T))%>%
#   summarise(correct = list(mean)) %>%
#   group_by(model)%>%
#   mutate(mean= mean(unlist(correct)),
#          sd = sd(unlist(correct)),
#          bf = extractBF(ttestBF(unlist(correct), mu = 1/2))$bf,
#          iter = i)%>%
#     select(-correct)
# 
# bf_chance <- bind_rows(x, bf_chance)
# }
# 
# 
# saveRDS(bf_chance, "../analysis/saves/chance_bfs.rds")

bf_chance <- readRDS("../analysis/saves/chance_bfs.rds")

bf_chnace_agg <- bf_chance%>%
  group_by(model)%>%
  summarise(mean_bf = mean(bf),
            mean = mean(mean))
```


First, we averaged across predictions for each participant and compared the proportion of correct predictions across participants to a level expected by random guessing (50% correct predictions). Figure \@ref(fig:chanceplot) shows  the results of one run of the coin-flip procedure. For the analysis, we repeated the coin-flip 1000 times and for each run we computed the mean and the Bayes Factor for the comaprison to chance for each model. The *rational integration* model made correct predictions at a level clearly above chance for both age groups (correct onn average in `r round(bf_chnace_agg%>%filter(model == "combination")%>%pull(mean),2)*100`% of cases, average Bayes Factor = `r format(bf_chnace_agg%>%filter(model == "combination")%>%pull(mean_bf),2)`). In a second step, we compared the predictions made by the *rational integration* model to those of the two lesioned models. For this, we generated trial and child specific model predictions for each of the lesioned models in the same ways as for the *rational integration* model. Here we saw that both models made predictions above chance (*no common ground* model: mean correct = `r round(bf_chnace_agg%>%filter(model == "flatPrior")%>%pull(mean),2)*100`%, mean BF = `r format(bf_chnace_agg%>%filter(model == "flatPrior")%>%pull(mean_bf),2)`; *no speaker informativeness* model: mean correct = `r round(bf_chnace_agg%>%filter(model == "priorOnly")%>%pull(mean),2)*100`%, mean BF = `r format(bf_chnace_agg%>%filter(model == "priorOnly")%>%pull(mean_bf),2)`), confirming that these were valuable alternative models. However, we also saw that the *rational integration* model was more likely to make correct predictions. 

```{r cache=T}
# mpred_comb <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "combination",
#          chain != 1, 
#          chain != 2)%>%
#   group_by(model, condition,familiar,subid)%>%
#   summarise(model_mean = estimate_mode(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# mpred_flat <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "flatPrior",
#          chain != 1, 
#          chain != 2)%>%
#   group_by(model, condition,familiar,subid)%>%
#   summarise(model_mean = estimate_mode(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# mpred_prior <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "priorOnly",
#          chain != 1, 
#          chain != 2)%>%
#   group_by(model, condition,familiar,subid)%>%
#   summarise(model_mean = estimate_mode(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# 
# mpred <- bind_rows(
#   mpred_comb,
#   mpred_flat,
#   mpred_prior
# )
#  
# saveRDS(mpred, "saves/id_model_pred.rds")

mpred <- readRDS("saves/id_model_pred.rds")

chance_id <- mpred%>%
  left_join(data %>% select(condition,subage,familiar,subid, correct))%>%
  filter(!is.na(subage))%>%
  #ungroup()%>%
  rowwise()%>%
  mutate(pred = flip(model_mean))%>%
  ungroup()%>%
  mutate(#match = ifelse(round(model_mean) == correct, 1, 0),
         match = ifelse(pred == correct, 1, 0))%>%
  group_by(subid, subage,model)%>%
  summarise(mean = mean(match, na.rm = T))%>%
  mutate(subage = ifelse(subage == 3, "3-year-olds", "4-year-olds"))

chance <- chance_id%>%
  group_by(subage,model)%>%
  tidyboot_mean(col = mean)


p_chance <- ggplot(chance, aes(x = model, y = mean))+
  geom_hline(yintercept = 0.5, lty = 2, alpha = .5)+
  geom_dotplot(data = chance_id, binaxis='y', stackdir='center',
               stackratio=1.5, dotsize=.5, alpha = .2)+
  geom_violin(data = bf_chance, aes(x = model, y = mean), alpha = 0.25)+
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper, col = model), size = 0.6)+
  ylim(0,1)+
  facet_grid(~subage)+
  labs(x = "", y = "Proportion correct predictions")+
  scale_color_manual(name = "Model",
                     labels=c("Integration","No Common Ground","No Speaker Informativeness"),
                     
                    values= c("#6a994e","#6e1423","#a71e34"))+
  theme_few()+
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())

```

```{r}
#ggsave("../graphs/within_model_pred.png", width = 6, height = 4, scale = 1.2)
```

```{r chanceplot, fig.cap = "Proportion of correct model predictions for the three models (one iteration of coin-flip procedure). Violins show distribution of means for 1000 runs of the procedure. Colored dots show mean for each model (with 95\\% CI) and light grey dots show participant means.",fig.height = 3.5, fig.width= 10, out.width="100%"}
p_chance
```

Finally, we looked at the "hits" and "misses", that is, which responses the model was more likely to predict correctly and which not. Figure \@ref(fig:hitplot) showed that the *rational integration* model was -- avergaed across 1000 runs of the coin-flip procedure -- correct in almost all cases when the outcome was `1`. When the outcome was `0`, it was correct in about 50% of cases.  

```{r}
# hpp <- tibble()
# 
# for (i in 1:1000) {
#   x <- mpred%>%
#   left_join(data %>% select(condition,subage,familiar,subid, correct))%>%
#   filter(!is.na(subage))%>%
#   rowwise()%>%
#   mutate(pred = flip(model_mean))%>%
#   ungroup()%>%
#   group_by(model, correct, pred)%>%
#   filter(!is.na(correct))%>%
#   summarise(n= n())%>%
#   group_by(model,correct)%>%
#   mutate(prop = n / (sum(n)),
#          Data = factor(correct, levels = c(0,1), labels = c("0", "1")),
#          Prediction = factor(pred, levels = c(0,1), labels = c("0", "1")), 
#          iter = i)
#   
#   hpp <- bind_rows(x,hpp)
#   
# }
# 
# hpp_agg <- hpp%>%
#   group_by(model, Data, Prediction)%>%
#   summarise(prop = mean(prop))%>%
#   mutate(model = factor(model, levels = c("combination", "flatPrior","priorOnly"),labels=c("Integration","No Common Ground","No Speaker Informativeness")))
# 
# saveRDS(hpp_agg, "../analysis/saves/hit_miss.rds")

hpp_agg <- readRDS("../analysis/saves/hit_miss.rds")

hp <- ggplot(hpp_agg,aes(x = Prediction, y = Data, fill = prop))+
  geom_tile(colour="black",size=0.5, aes(size = n))+
  facet_grid(~model)+
  #coord_fixed()+
  theme_few()+
  scale_fill_gradient(low = "white", high = "black", limit = c(0,1), name="Proportion")
```

```{r hitplot, fig.cap = "Proportion of correct predicitons by model for the two possible outcomes in the data.",fig.height = 3.5, fig.width= 10, out.width="100%"}
hp
```

```{r}
# idsk <- model_parameters%>%
#   rename(parameter = c,
#          familiar = d,
#          subid = e,
#          value = f)%>%
#   filter(!is.na(familiar),
#          parameter == "semantic_knowledge",
#          familiar == "rasp",
#          subid == 13028)
# 
# idp <- model_parameters%>%
#   rename(parameter = c,
#          familiar = d,
#          subid = e,
#          value = f)%>%
#   filter(parameter == "prior",
#          subid == 13028)
# 
# idso <- model_parameters%>%
#   rename(parameter = c,
#          familiar = d,
#          subid = e,
#          value = f)%>%
#   filter(parameter == "speaker_optimality",
#          subid == 13028)
# 
# idc <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "combination", familiar == "rasp", subid == 13028, condition == "incongruent")
# 
# 
# idf <- model_predictions%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)%>%
#   filter(model == "flatPrior", familiar == "rasp", subid == 13028, condition == "incongruent")
# #6e1423
# 
# idf %>%
#   ggplot(aes(x = pred))+
#   #geom_density(alpha = .25, fill = "#6a994e")+
#   geom_density(alpha = .25, fill = "#6e1423")+
#   scale_fill_ptol()+
#   theme_few()+
#   #labs(x = "Propability novel object", y = "")+
#   labs(x = "Probability novel object", y = "")+
#   xlim(0,1)+
#   theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())

```

```{r}
#ggsave("../graphs/id_pred_flat.png", width = 4, height = 4, scale = 1)
```

### Participant specific Bayes factors

```{r parytayes, }
id_bf <- readRDS("./saves/id_bf.rds")%>%
  left_join(data%>%select(subid,subage, age)%>%rename(id = subid)%>%distinct(id, .keep_all = T))

id_bf_comp <- id_bf%>%
  group_by(Comparison)%>%
  summarise(prop_pos_BF = sum(log_BF > 0)/60,
            prop_BF_3 = sum(log_BF > 1)/60,
            prop_BF_10 = sum(log_BF > 2.3)/60,
            prop_BF_neg3 = sum(log_BF < -1)/60,
            prop_BF_neg10 = sum(log_BF < -2.3)/60)
```

For this final analysis, we repeated the model comparison on an individual level. That is, for each participant, we computed a Bayes factor (BF) in favor of the *rational integration* model compared to the two lesioned models. Figure \@ref(fig:idbfplot)A visualizes the distribution of the resulting BFs on the log-scale. Log-BFs larger than 0 correspond to BFs > 1 and thus suggest that the *rational integration* model fit the data better compared to the other model for this participant. Taken together, `r id_bf_comp%>%filter(Comparison == "bf_comb_flat")%>%pull(prop_pos_BF)`% of log-BFs were larger than 0 when comparing the *rational integration* model to the *no common ground* model and `r id_bf_comp%>%filter(Comparison == "bf_comb_prior")%>%pull(prop_pos_BF)` % of log-BFs were larger than 0 when comparing the *rational integration* model to the *no speaker informativeness* model. Taken together, this suggests that for the majority of participants, the *rational integration* model was the best model to predict their behavior in a new situation. Figure \@ref(fig:idbfplot)B 

```{r}
pidbf <- ggarrange(  
id_bf%>%
  dplyr::mutate(Comparison = ifelse(Comparison == "bf_comb_flat", "integration vs. no common ground", "integration vs. no speaker informativeness"))%>%
  ggplot(aes(x = log_BF, fill = factor(Comparison)))+
  geom_vline(xintercept = 0, lty = 1, col = "black")+
  geom_vline(xintercept = c(log(3),log(10), log(100)), lty = 2, alpha = .75, col = "#7a942e")+
  geom_vline(xintercept = c(log(0.3),log(0.1), log(0.01)), lty = 2, alpha = .75, col = "firebrick")+
  geom_density(alpha = .5)+
  #facet_grid(~Comparison)+
  geom_point(aes(y = 0, col = factor(Comparison)), pch = "I", size = 5)+
  theme_few()+
  labs(x = "log(BF) favor of integration model", y = "Density")+
  scale_fill_colorblind(name = "")+
  scale_color_colorblind(name = "")+
  theme(legend.position = "top")
,
id_bf%>%
  dplyr::mutate(Comparison = ifelse(Comparison == "bf_comb_flat", "integration vs. no common ground", "integration vs. no speaker informativeness"))%>%
  ggplot(aes(x = age+3, col = Comparison, y = log_BF))+
  geom_point(alpha = .75, pch = 1)+
  geom_smooth(method = "lm")+
  labs(y = "log(BF) favor of integration model", x = "Age")+
  scale_color_colorblind(name = "")+
  stat_cor(r.accuracy = 0.01, cor.coef.name = "r", show.legend = FALSE)+
  theme_minimal(),
common.legend = T,
legend = "top",
labels = c("A","B")
)
  
  
```

```{r idbfplot, fig.cap = "A: distribution of log Bayes Facotrs in favor of the integration model compared to the two lesion models. B: relation between age and log Bayes Factor in favor of the intergration model.",fig.height = 3.5, fig.width= 10, out.width="100%"}
pidbf
```


# Appendix 1: Group model 

In addition, we explored an alternative way of generating the participant specific predictions. Instead of using the participant specific parameters, we used the age specific hype parameters. In the section on [model parameters](#model-parameters) we described how the participant specific parameters were estimated as deviations from age specific hyper parameters (i.e. values generated by an age sensitive slope and intercept). This alternative model uses parameters that were one level up in our hierarchical model. The reasoning was that these parameters might actually lead to better predictions, because they are estimated based on more data. 

Figure \@ref(fig:chancecompid) shows that this was not the case. The participant specific model was slightly more likely to make correct predictions compared to the group-level model. 

```{r}
# model_predictions_global <- bind_rows(
#   readRDS("../model/output/comb_model_global_predictions_1.rds"),
#   readRDS("../model/output/comb_model_global_predictions_2.rds"),
#   readRDS("../model/output/comb_model_global_predictions_3.rds"),
#   readRDS("../model/output/comb_model_global_predictions_4.rds"),
#   readRDS("../model/output/comb_model_global_predictions_5.rds")
#   )
```

```{r, cache=T}
# mpred_all_global <- model_predictions_global%>%
#   filter(chain != 2)%>%
#   select(-a, -g)%>%
#   rename(model = b,
#          condition = c,
#          familiar = d,
#          subid = e,
#          pred = f)
# 
# mpred_comb_global <- mpred_all_global%>%
#   filter(model == "combination")%>%
#   group_by(model, condition,familiar,subid)%>%
#   summarise(model_mean = estimate_mode(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# mpred_flat_global <- mpred_all_global%>%
#   filter(model == "flatPrior")%>%
#   group_by(model, condition,familiar,subid)%>%
#   summarise(model_mean = estimate_mode(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# mpred_prior_global <- mpred_all_global%>%
#   filter(model == "priorOnly")%>%
#   group_by(model, condition,familiar,subid)%>%
#   summarise(model_mean = estimate_mode(pred),
#             model_ci_lower = hdi_lower(pred),
#             model_ci_upper = hdi_upper(pred))
# 
# 
# mpred_global <- bind_rows(
#   mpred_comb_global,
#   mpred_flat_global,
#   mpred_prior_global
# )
# 
# saveRDS(mpred_global, "saves/id_model_pred_global.rds")

mpred_global <- readRDS("saves/id_model_pred_global.rds")

chance_id_global <- mpred_global%>%
  left_join(data %>% select(condition,subage,familiar,subid, correct))%>%
  filter(!is.na(subage))%>%
  #ungroup()%>%
  rowwise()%>%
  mutate(pred = flip(model_mean))%>%
  ungroup()%>%
  mutate(#match = ifelse(round(model_mean) == correct, 1, 0),
         match = ifelse(pred == correct, 1, 0))%>%
  group_by(subid, subage,model)%>%
  summarise(mean = mean(match, na.rm = T))%>%
  mutate(subage = ifelse(subage == 3, "3-year-olds", "4-year-olds"))

chance_global <- chance_id_global%>%
  group_by(subage,model)%>%
  tidyboot_mean(col = mean)
```

## Comparison ID vs. Group model 

```{r, cache=T}
chance_id_comp <- bind_rows(
  chance_id_global%>%mutate(type = "Group"),
  chance_id%>%mutate(type = "ID") 
  )%>%
  mutate(model = factor(model, levels = c("combination", "flatPrior","priorOnly"),labels=c("Integration","No Common Ground","No Speaker Informativeness")))

chance_comp <- bind_rows(
  chance_global%>%mutate(type = "Group"),
  chance%>%mutate(type = "ID")
)%>%
  mutate(model = factor(model, levels = c("combination", "flatPrior","priorOnly"),labels=c("Integration","No Common Ground","No Speaker Informativeness")))


p_comp_id <- ggplot(chance_comp, aes(x = model, y = mean))+
  geom_hline(yintercept = 0.5, lty = 2, alpha = .5)+
  geom_point(data = chance_id_comp,aes(x = model, y = mean, col = type), alpha = .2, position = position_dodge(width = .9))+
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper, col = type), size = 0.6, position = position_dodge(width = .9))+
  ylim(0,1)+
  facet_grid(~subage)+
  labs(x = "", y = "Proportion correct predictions")+
  scale_color_few(name = "Model type")+
  theme_few()+
  scale_x_discrete(guide = guide_axis(n.dodge=3))
```

```{r chancecompid, cache=T, fig.cap = "Proportion of correct model predictions for the two versions of the integration model. Group refers to predictions based on age specific hyper parameters and ID to predictions based on participant specific parameters. Colored dots show mean for each model (with 95\\% CI) and light colored dots show participant means.",fig.height = 4, fig.width= 10, out.width="100%"}
p_comp_id
```

# Appendix 2: Model parameters

Below we visualize the posterior distributions for some of the parameters in the model.

## Speaker informativeness

```{r, cache=T}
# speak_opt <- model_parameters%>%
#   filter(c == "speaker_optimality", 
#          b != "parameters",
#          chain != 1, 
#          chain != 2)%>%
#   rename(parameter = b,
#          value = f)%>%
#   mutate(parameter = factor(parameter, levels = c("intercept", "slope", "sigma")))
# 
# saveRDS(speak_opt, "saves/speak_opt_plot.rds")

speak_opt <- readRDS("saves/speak_opt_plot.rds")

p_speak_opt <- ggplot(speak_opt, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = .5)+
  facet_grid(~parameter)+
  scale_fill_colorblind(name = "Chain")+
  xlab("Parameter estimate")+
  ylab("")+
  #ggtitle("Speaker informativeness (hyper parameters)")+
  xlim(-4,4)+
  theme_minimal()
```

```{r, cache=T}
# speak_opt_id <- model_parameters%>%
#   rename(type = b,
#          parameter = c,
#          subject = e, 
#          speaker_optimality = f)%>%
#   filter(type != "sigma",
#          parameter == "speaker_optimality",
#          chain != 1, 
#          chain != 2)%>%
#   mutate(subject = factor(subject))%>%
#   filter(subject %in% sample(levels(subject),5))
# 
# saveRDS(speak_opt_id, "saves/speak_opt_id_plot.rds")

speak_opt_id <- readRDS("saves/speak_opt_id_plot.rds")

p_speak_opt_id <- ggplot(speak_opt_id, aes(x = speaker_optimality, fill = factor(subject)))+
  geom_density(alpha = .5)+
  #ggtitle("Speaker informativeness for 5 randomly selected individuals")+
  #facet_grid(~subject, scales = "free_y")+
  ylab("")+
  xlab("Speaker informativeness expectation")+
  scale_fill_viridis_d(name = "Participant ID")+
  theme_minimal()
```

```{r speakoptplot, cache=T, fig.cap = "Model parameters for speaker informativeness. A) hyper parameters colored by MCMC chain and B) participant specific parameters for five randomly selected individuals.",fig.height = 3, fig.width= 10, out.width="100%"}
ggarrange(p_speak_opt, p_speak_opt_id, common.legend = F, nrow = 1, widths = c(1.5,1), labels = c("A","B"))
```

## Semantic knowledge

```{r, cache=T}
# sem_know <- model_parameters%>%
#   filter(c == "semantic_knowledge", 
#          b != "parameters",
#          chain != 1, 
#          chain != 2)%>%
#   rename(parameter = b,
#          value = f)%>%
#   mutate(parameter = factor(parameter, levels = c("intercept", "slope", "sigma")))
# 
# saveRDS(sem_know, "saves/sem_know_plot.rds")

sem_know <- readRDS("saves/sem_know_plot.rds")

p_sem_know <- ggplot(sem_know, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = .5)+
  facet_grid(~parameter)+
  scale_fill_colorblind(name = "Chain")+
  ylab("")+
  xlab("Parameter estimate")+
  xlim(-4,4)+
  theme_minimal()
```

```{r, cache=T}
# sem_know_id <- model_parameters%>%
#   rename(parameter = c,
#          familiar = d, 
#          subject = e, 
#          value = f)%>%
#   filter(!is.na(familiar),
#          parameter == "semantic_knowledge", 
#          familiar == "duck" | familiar == "thermo",
#          chain != 1, 
#          chain != 2)%>%
#   mutate(subject = factor(subject))%>%
#   filter(subject %in% sample(levels(subject),5))
# 
# saveRDS(sem_know_id, "saves/sem_know_id_plot.rds")

sem_know_id <- readRDS("saves/sem_know_id_plot.rds")

p_sem_know_id <- ggplot(sem_know_id, aes(x = value, fill = factor(subject)))+
  geom_density(alpha = .5)+
  #ggtitle("Semantic knowledge for 'lock'")+
  facet_grid(familiar~., scales = "free_y")+
  ylab("")+
  xlab("Semantic knowledge")+
  scale_fill_viridis_d(name = "Participant ID")+
  theme_minimal()
```

```{r semknowplot, cache=T, fig.cap = "Model parameters for semantic knowledge. A) hyper parameters colored by MCMC chain and B) participant specific parameters for five randomly selected individuals for two familiar objects.",fig.height = 3, fig.width= 10, out.width="100%"}
ggarrange(p_sem_know, p_sem_know_id, common.legend = F, nrow = 1, widths = c(1.5,1), labels = c("A","B"))
```

```{r}
# ### by item
# model_parameters%>%
#   rename(parameter = c,
#          familiar = d, 
#          subject = e, 
#          value = f)%>%
#   filter(!is.na(familiar),
#          !is.na(value),
#          !is.na(subject),
#          parameter == "semantic_knowledge",
#          chain != 1, 
#          chain != 2)%>%
#   group_by(subject, familiar,chain)%>%
#     summarise(mode = estimate_mode(value),
#             lci = hdi_lower(value),
#             uci = hdi_upper(value))%>%
#   ggplot(., aes(x = factor(subject), y= mode, col = chain))+
#   geom_pointrange(aes(ymin = lci, ymax = uci), position = position_dodge(width = .1), pch = 4)+
#   scale_color_colorblind()+
#   facet_grid(familiar~.)+
#   theme_few()
```

## Sensitivity to common ground

```{r, cache=T}
# cg <- model_parameters%>%
#   filter(c == "prior", 
#          b != "parameters",
#          chain != 1, 
#          chain != 2)%>%
#   rename(parameter = b,
#          value = f)%>%
#   mutate(parameter = factor(parameter, levels = c("intercept", "slope", "sigma")))
# 
# saveRDS(cg, "saves/com_ground_plot.rds")

cg <- readRDS("saves/com_ground_plot.rds")

p_cg <- ggplot(cg, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = .5)+
  ylab("")+
  #ggtitle("Sensitivity to common ground (hyper parameter)")+
  facet_grid(~parameter)+
  ylab("")+
  xlab("Parameter estimate")+
  scale_fill_colorblind(name = "Chain")+
  xlim(-4,4)+
  theme_minimal()
```

```{r, cache=T}
# cg_id <- model_parameters%>%
#   rename(parameter = c, 
#          familiar = d, 
#          subject = e, 
#          value = f)%>%
#   filter(!is.na(subject),
#          parameter == "prior",
#          chain != 1, 
#          chain != 2)%>%
#   mutate(subject = factor(subject))%>%
#   filter(subject %in% sample(levels(subject),5))
# 
# saveRDS(cg_id, "saves/com_ground_id_plot.rds")

cg_id <- readRDS("saves/com_ground_id_plot.rds")

p_cg_id <- ggplot(cg_id, aes(x = value, fill = factor(subject)))+
  geom_density(alpha = .5)+
  #ggtitle("Sensitivity to common ground for 5 randomly selected individuals")+
  #facet_grid(~subject, scales = "free_y")+
  ylab("")+
  xlab("Sensitivity to common ground")+
  scale_fill_viridis_d(name = "Participant ID")+
  xlim(0,1)+
  theme_minimal()

```

```{r cgplot, cache=T, fig.cap = "Model parameters for sensitivity to common ground. A) hyper parameters colored by MCMC chain and B) participant specific parameters for five randomly selected individuals.",fig.height = 3, fig.width= 10, out.width="100%"}
ggarrange(p_cg, p_cg_id, common.legend = F, nrow = 1, widths = c(1.5,1), labels = c("A","B"))
```

## Production probability

This parameter was part of the production model and gives the probability that the child was able to produce a word if their semantic knowledge indicates that they knew it. This parameter was the same for all participants. 

```{r, cache=T}
# pod_prob <- model_parameters%>%
#   filter(c == "successful_production_probability",
#          chain != 1, 
#          chain != 2)%>%
#   rename(value = f)
# 
# saveRDS(pod_prob, "saves/prod_prob_plot.rds")

pod_prob <- readRDS("saves/prod_prob_plot.rds")

p_pod_prob <- ggplot(pod_prob, aes(x = value, fill = factor(chain)))+
  geom_density(alpha = .5)+
  ylab("")+
  xlab("Successful production probability")+
  #ggtitle("Production Probability")+
  scale_fill_colorblind(name = "Chain")+
  xlim(0,1)+
  theme_minimal()
```


```{r prodprob, cache=T, fig.cap = "Model parameter for production probability colored by MCMC chain",fig.height = 3, fig.width= 6, out.width="50%"}
p_pod_prob
```

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
